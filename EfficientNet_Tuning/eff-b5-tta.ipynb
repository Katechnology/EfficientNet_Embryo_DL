{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54000,"databundleVersionId":6617141,"sourceType":"competition"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import albumentations\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T11:59:47.311085Z","iopub.execute_input":"2023-12-11T11:59:47.311353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/world-championship-2023-embryo-classification/hvwc23/train.csv')\ndata_train = pd.DataFrame()\ndata_train['path'] = '/kaggle/input/world-championship-2023-embryo-classification/hvwc23/train/' + train['Image']\ndata_train['label'] = train['Class']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntest = pd.read_csv('/kaggle/input/world-championship-2023-embryo-classification/hvwc23/test.csv')\ndata_test = pd.DataFrame()\ndata_test['path'] = '/kaggle/input/world-championship-2023-embryo-classification/hvwc23/test/' + test['Image']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.label.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# => data is imbalance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_path = '/kaggle/input/world-championship-2023-embryo-classification/hvwc23/train'\ntest_img_path = '/kaggle/input/world-championship-2023-embryo-classification/hvwc23/test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n    \n# define a data class\nclass ClassificationDataset:\n    def __init__(self, data, data_path, transform, training=True):\n        \"\"\"Define the dataset for classification problems\n\n        Args:\n            data ([dataframe]): [a dataframe that contain 2 columns: image name and label]\n            data_path ([str]): [path/to/folder that contains image file]\n            transform : [augmentation methods and transformation of images]\n            training (bool, optional): []. Defaults to True.\n        \"\"\"\n        self.data = data\n        self.imgs = data[\"path\"].unique().tolist()\n        self.data_path = data_path\n        self.training = training\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.data_path, self.data.iloc[idx, 0]))\n        if(self.training):\n            label = self.data.iloc[idx, 1]\n        if self.transform is not None:\n            img = self.transform(img)\n        if(self.training):\n            return img, label\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef make_loader(dataset, train_batch_size, validation_split=0.2):\n    \"\"\"make dataloader for pytorch training\n\n    Args:\n        dataset ([object]): [the dataset object]\n        train_batch_size ([int]): [training batch size]\n        validation_split (float, optional): [validation ratio]. Defaults to 0.2.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    # number of samples in train and test set\n    train_len = int(len(dataset) * (1 - validation_split))\n    test_len = len(dataset) - train_len\n    train_set, test_set = torch.utils.data.random_split(dataset, [train_len, test_len])\n    # create train_loader\n    print(len(train_set))\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=train_batch_size, shuffle=True,\n    )\n    # create test_loader\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False,)\n    return train_loader, test_loader\n\n\ndef data_split(data, test_size):\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, data[\"label\"], test_size=test_size, stratify = data.iloc[:,1]\n    )\n    return x_train, x_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Define DataLoader","metadata":{}},{"cell_type":"code","source":"mean = (0.5, 0.5, 0.5)\nstd = (0.5, 0.5, 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\n# transform = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)),\n#                                             torchvision.transforms.RandomRotation(20),\n#                                             torchvision.transforms.RandomHorizontalFlip(0.1),\n#                                             torchvision.transforms.RandomAffine(degrees=11, translate=(0.1,0.1), scale=(0.8,0.8)),\n#                                             torchvision.transforms.ToTensor(),\n#                                             torchvision.transforms.Normalize(mean, std)])\n\ntransform = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)),\n                                               torchvision.transforms.ToTensor(),\n                                               torchvision.transforms.Normalize(mean, std)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transform = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)),\n                                               torchvision.transforms.ToTensor(),\n                                               torchvision.transforms.Normalize(mean, std)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = 16 # batch size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ClassificationDataset(data_train,data_path = \"\",transform=transform,training=True)\ntrain_loader,val_loader = make_loader(dataset, train_batch_size=bs, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" X_train, X_test, y_train, y_test = train_test_split(data_train, data_train['label'],stratify=data_train['label'], test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClassificationDataset(X_train,data_path = \"\",transform=transform,training=True)\nval_dataset = ClassificationDataset(X_test,data_path = \"\",transform=test_transform,training=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=bs, shuffle=True,\n    )\n    # create test_loader\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset = ClassificationDataset(data_test,data_path = \"\",transform=test_transform,training=False)\ntest_loader = torch.utils.data.DataLoader(\n        testset, batch_size=1, shuffle=False,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\npsum    = torch.tensor([0.0, 0.0, 0.0])\npsum_sq = torch.tensor([0.0, 0.0, 0.0])\n\n# loop through images\nfor inputs in tqdm(test_loader):\n    psum    += inputs.sum(axis        = [0, 2, 3])\n    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n\ncount = len(data_test) * 224 * 224\n\n# mean and std\ntest_mean = psum / count\ntest_var  = (psum_sq / count) - (test_mean ** 2)\ntest_std  = torch.sqrt(test_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"psum    = torch.tensor([0.0, 0.0, 0.0])\npsum_sq = torch.tensor([0.0, 0.0, 0.0])\n\n# loop through images\nfor inputs,_ in tqdm(train_loader):\n    psum    += inputs.sum(axis        = [0, 2, 3])\n    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n\ncount = len(train_loader) * 224 * 224\n\n# mean and std\ntrain_mean = psum / count\ntrain_var  = (psum_sq / count) - (train_mean ** 2)\ntrain_std  = torch.sqrt(train_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Define Metrics and Optimizers and Loss function\n","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics as skmetrics\nimport numpy\nclass Metrics:\n    def __init__(self, metric_names):\n        self.metric_names = metric_names\n        # initialize a metric dictionary\n        self.metric_dict = {metric_name: [0] for metric_name in self.metric_names}\n\n    def step(self, labels, preds):\n        for metric in self.metric_names:\n            # get the metric function\n            do_metric = getattr(\n                skmetrics, metric, \"The metric {} is not implemented\".format(metric)\n            )\n            # check if metric require average method, if yes set to 'micro' or 'macro' or 'None'\n            try:\n                self.metric_dict[metric].append(\n                    do_metric(labels, preds, average=\"macro\")\n                )\n            except:\n                self.metric_dict[metric].append(do_metric(labels, preds))\n\n    def epoch(self):\n        # calculate metrics for an entire epoch\n        avg = [sum(metric) / (len(metric) - 1) for metric in self.metric_dict.values()]\n        metric_as_dict = dict(zip(self.metric_names, avg))\n        return metric_as_dict\n\n    def last_step_metrics(self):\n        # return metrics of last steps\n        values = [self.metric_dict[metric][-1] for metric in self.metric_names]\n        metric_as_dict = dict(zip(self.metric_names, values))\n        return metric_as_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_metrics = Metrics([\"accuracy_score\",\"f1_score\"])\nval_metrics = Metrics([\"accuracy_score\",\"f1_score\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nfrom torch import nn\n# criterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch.nn.functional as F\n# import torch.nn as nn\n\n# class LabelSmoothingLoss(nn.Module):\n#     def __init__(self, smoothing=0.1, dim=-1):\n#         super(LabelSmoothingLoss, self).__init__()\n#         self.smoothing = smoothing\n#         self.dim = dim\n\n#     def forward(self, pred, target):\n#         target = F.one_hot(target, num_classes=pred.size(-1))\n#         target = target.float()\n#         target = (1 - self.smoothing) * target + self.smoothing / pred.size(-1)\n#         log_pred = F.log_softmax(pred, dim=self.dim)\n#         loss = nn.KLDivLoss(reduction='batchmean')(log_pred, target)\n#         return loss\n    \n# criterion = LabelSmoothingLoss(smoothing=0.12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(data_train.label),\n                                        y = data_train.label                                                    \n                                    )\n#class_weights = dict(zip(np.unique(data_train.label), class_weights))\nclass_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\nclass_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Define the Model: Transfer Learning\n","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nfrom torch import nn\nmodel = models.efficientnet_b5(pretrained=True).cuda()\nfor param in model.parameters():\n    param.requires_grad = True\nclassifier = nn.Sequential(\n    nn.Linear(in_features=model.classifier[1].in_features, out_features=256,bias=True),\n    nn.Linear(in_features=256, out_features=2,bias=True)\n)\nmodel.classifier  = classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \"min\", patience=2, factor=0.5\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Define a training epoch\n","metadata":{}},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(\n    model,\n    train_loader,\n    test_loader,\n    device,\n    optimizer,\n    criterion,\n    train_metrics,\n    val_metrics,\n):\n\n    # training-the-model\n    train_loss = 0\n    valid_loss = 0\n    all_labels = []\n    all_preds = []\n    model.train()\n    for data, target in train_loader:\n        # move-tensors-to-GPU\n        data = data.type(torch.FloatTensor).to(device)\n        # target=torch.Tensor(target)\n        target = target.float().to(device)\n        # clear-the-gradients-of-all-optimized-variables\n        optimizer.zero_grad()\n        # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n        output = model(data)\n        # get the prediction label and target label\n        output = model(data)\n        preds = torch.argmax(output, axis=1).cpu().detach().numpy()\n        labels = target.cpu().numpy()\n        # calculate-the-batch-loss\n        loss = criterion(output.type(torch.FloatTensor).to(device), target.type(torch.LongTensor).to(device))\n        # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n        loss.backward()\n        # perform-a-ingle-optimization-step (parameter-update)\n        optimizer.step()\n        # update-training-loss\n        train_loss += loss.item() * data.size(0)\n        # calculate training metrics\n        all_labels.extend(labels)\n        all_preds.extend(preds)\n    \n    train_metrics.step(all_labels, all_preds)\n\n    # validate-the-model\n    model.eval()\n    all_labels = []\n    all_preds = []\n    for data, target in test_loader:\n        data = data.type(torch.FloatTensor).to(device)\n        target = target.to(device)\n        output = model(data)\n        preds = torch.argmax(output, axis=1).tolist()\n        labels = target.tolist()\n        all_labels.extend(labels)\n        all_preds.extend(preds)\n        loss = criterion(output, target)\n\n        # update-average-validation-loss\n        valid_loss += loss.item() * data.size(0)\n\n    val_metrics.step(all_labels, all_preds)\n    train_loss = train_loss / len(train_loader.sampler)\n    valid_loss = valid_loss / len(test_loader.sampler)\n\n    return (\n        train_loss,\n        valid_loss,\n        train_metrics.last_step_metrics(),\n        val_metrics.last_step_metrics(),\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom datetime import datetime\ntime_str = str(datetime.now().strftime(\"%Y%m%d-%H%M\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n\nmodel = model.to(device)\nnum_epoch = 20\nbest_val_acc = 0.0\nimport logging\nimport numpy as np\nprint(\"begin training process\")\nfor i in tqdm(range(0, num_epoch)):\n    loss, val_loss, train_result, val_result = train_one_epoch(\n        model,\n        train_loader,\n        val_loader,\n        device,\n        optimizer,\n        criterion,\n        train_metrics,\n        val_metrics,\n    )\n\n    scheduler.step(val_loss)\n    print(\n        \"Epoch {} / {} \\n Training loss: {} - Other training metrics: \".format(\n            i + 1, num_epoch, loss\n        )\n    )\n    print(train_result)\n    print(\n        \" \\n Validation loss : {} - Other validation metrics:\".format(val_loss)\n    )\n    print(val_result)\n    print(\"\\n\")\n    # saving epoch with best validation accuracy\n    if (loss<0.04):\n        # no saving\n        continue\n    if best_val_acc < float(val_result[\"accuracy_score\"]):\n        print(\n            \"Validation accuracy= \"+\n            str(val_result[\"accuracy_score\"])+\n            \"===> Save best epoch\"\n        )\n        best_val_acc = val_result[\"accuracy_score\"]\n        torch.save(\n            model,\n            \"./\" +  \"best.pt\"\n        )\n    else:\n        print(\n            \"Validation accuracy= \"+ str(val_result[\"accuracy_score\"])+ \"===> No saving\"\n        )\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lion-pytorch\nfrom lion_pytorch import Lion\noptimizer = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\ntest_model = torch.load(\"/kaggle/working/best.pt\")\ntest_model = test_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# threshold = 0.4 \n\ndef test_result(model, test_loader, device,name='no_tta_prob.npy'):\n    # testing the model by turning model \"Eval\" mode\n    model.eval()\n    preds = []\n    aprobs = []\n    with torch.no_grad():\n        for data in test_loader:\n            # move-tensors-to-GPU\n            data = data.to(device)\n            # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n            output = model(data)\n            prob = nn.Softmax(dim=1)\n            # applying Softmax to results\n            probs = prob(output)\n            aprobs.append(probs.cpu())\n            preds.extend(torch.argmax(probs, axis=1).tolist())\n#             binary_predictions = (probs[:, 1] >= threshold).int()\n#             preds.extend(binary_predictions.tolist())\n    aprobs = np.array(aprobs)\n    np.save(name,aprobs)\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds =test_result(test_model, test_loader, device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/world-championship-2023-embryo-classification/hvwc23/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Class'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission_new.csv\",index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ttach\nimport ttach as tta\ntest_model = torch.load(\"/kaggle/working/best.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = tta.Compose(\n    [\n        tta.HorizontalFlip(),\n        tta.Rotate90(angles=[0, 180]),\n        tta.Multiply(factors=[0.9, 1, 1.1]),        \n    ]\n)\n\ntta_model = tta.ClassificationTTAWrapper(test_model, transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submission['Class'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds =test_result(tta_model, test_loader, device,name='tta_prob.npy')\nsubmission['Class'] = preds\nsubmission.to_csv(\"submission_tta.csv\",index=None)\nprint(submission['Class'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds =test_result(tta_model, test_loader, device,name='tta_prob.npy')\nsubmission['Class'] = preds\nsubmission.to_csv(\"submission.csv\",index=None)\nprint(submission['Class'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}